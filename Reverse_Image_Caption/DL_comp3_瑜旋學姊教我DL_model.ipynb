{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories\n",
    "CHECKPOINT_DIR = './checkpoints/final_version_3'\n",
    "SAMPLE_DIR = './samples/final_version'\n",
    "INFERENCE_DIR = './inference/final_version'\n",
    "\n",
    "# data\n",
    "DATASET_REPETITIONS = 5\n",
    "NUM_EPOCHS = 300  \n",
    "IMAGE_SIZE = 64\n",
    "MAX_TEXT_LENGTH  = 77\n",
    "PLOT_DIFFUSION_STEPS = 50\n",
    "\n",
    "# sampling\n",
    "MIN_SIGNAL_RATE = 0.02\n",
    "MAX_SIGNAL_RATE = 0.95\n",
    "\n",
    "# architecture\n",
    "EMBEDDING_DIMENSIONS = 32\n",
    "EMBEDDING_MAX_FREQUENCY = 1000.0\n",
    "WIDTHS = [64, 96, 128, 160]\n",
    "BLOCK_DEPTH = 2\n",
    "HEAD = 2\n",
    "\n",
    "# optimization\n",
    "BATCH_SIZE = 64\n",
    "EMA = 0.999\n",
    "WEIGHT_DECAY = 1e-4\n",
    "START_EMA = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAMPLE_DIR):\n",
    "    os.makedirs(SAMPLE_DIR)\n",
    "\n",
    "if not os.path.exists(INFERENCE_DIR):\n",
    "    os.makedirs(INFERENCE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7b04a",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13ebc1",
   "metadata": {},
   "source": [
    "## Create Dataset by Dataset API\n",
    "* Tokenizer: https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db251bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "from src.clip_tokenizer import SimpleTokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "def training_data_generator(captions, image_path):\n",
    "    # Load and preprocess image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "\n",
    "    # Crop and resize image\n",
    "    height, width = tf.shape(img)[0], tf.shape(img)[1]\n",
    "    crop_size = tf.minimum(height, width)\n",
    "    img = tf.image.crop_to_bounding_box(img, (height - crop_size) // 2, (width - crop_size) // 2, crop_size, crop_size)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.resize(img, size=[IMAGE_SIZE, IMAGE_SIZE], antialias=True)\n",
    "    img = tf.clip_by_value(img / 255.0, 0.0, 1.0)\n",
    "    \n",
    "    # Select random caption\n",
    "    idx = tf.random.uniform(shape=(1,), minval=0, maxval=10, dtype=tf.int32)\n",
    "    caption = tf.gather(captions, idx)[0]\n",
    "    \n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "\n",
    "    for i in range(len(captions)):\n",
    "        img_caption = []\n",
    "        img_raw_caption_id = captions[i]\n",
    "        for cap_id in img_raw_caption_id:\n",
    "            img_raw_words = [id2word_dict[str(c_id.astype(np.int32))] for c_id in cap_id if c_id != '5427']\n",
    "            img_raw_caption = \" \".join(img_raw_words)\n",
    "            img_caption_id = tokenizer.encode(img_raw_caption)\n",
    "            phrase = img_caption_id + [49407] * (MAX_TEXT_LENGTH - len(img_caption_id))\n",
    "            img_caption.append(phrase)\n",
    "        while len(img_caption) < 10:\n",
    "            img_caption.append(random.choice(img_caption))\n",
    "        caption.append(img_caption)\n",
    "        \n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int32)\n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\\\n",
    "                             .map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                             .cache()\\\n",
    "                             .repeat(DATASET_REPETITIONS)\\\n",
    "                             .shuffle(10 * BATCH_SIZE)\\\n",
    "                             .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                             .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c59969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clip_encoder import CLIPTextTransformer\n",
    "\n",
    "def get_text_encoder():\n",
    "    input_word_ids = layers.Input(shape=(MAX_TEXT_LENGTH,), dtype=\"int32\")\n",
    "    input_pos_ids = layers.Input(shape=(MAX_TEXT_LENGTH,), dtype=\"int32\")\n",
    "    embeds = CLIPTextTransformer()([input_word_ids, input_pos_ids])\n",
    "    text_encoder = keras.models.Model([input_word_ids, input_pos_ids], embeds)\n",
    "    text_encoder.trainable = False\n",
    "    text_encoder_weights_fpath = keras.utils.get_file(\n",
    "        origin=\"https://huggingface.co/fchollet/stable-diffusion/resolve/main/text_encoder.h5\",\n",
    "        file_hash=\"d7805118aeb156fc1d39e38a9a082b05501e2af8c8fbdc1753c9cb85212d6619\",\n",
    "    )\n",
    "    text_encoder.load_weights(text_encoder_weights_fpath)\n",
    "    return text_encoder\n",
    "\n",
    "text_encoder = get_text_encoder()\n",
    "# text_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92bb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids_one = np.array(list(range(MAX_TEXT_LENGTH)))[None].astype(\"int32\")\n",
    "pos_ids = np.repeat(pos_ids_one, BATCH_SIZE, axis=0)\n",
    "test_pos_ids = np.repeat(pos_ids_one, 10, axis=0)\n",
    "uncondition_cap_one = np.array([[49406]+[49407]*(MAX_TEXT_LENGTH-1)])\n",
    "uncondition_caps = np.repeat(uncondition_cap_one, BATCH_SIZE, axis=0)\n",
    "test_uncondition_caps = np.repeat(uncondition_cap_one, 10, axis=0)\n",
    "uncondition_caps_emb = text_encoder([uncondition_caps, pos_ids])\n",
    "train_dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f8dda",
   "metadata": {},
   "source": [
    "## MODEL(DDIM)\n",
    "\n",
    "* Reference: https://keras.io/examples/generative/ddim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67846f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    " 'this white and purple flower has fragile petals and soft stamens',\n",
    " 'this flower has four large wide pink petals with white centers and vein like markings',\n",
    " 'a flower with broad white and pink ribbed petals and yellow stamen',\n",
    " 'one prominet pistil with alarger stigam and many stamens with anthers',\n",
    " 'leaves are green in color petals are light pink in color',\n",
    " 'this flower is bright pink with overlapping petals and a lime green pistil',\n",
    " 'this flower is white and yellow in color with petals that are multi colored',\n",
    " 'this flower has 4 leaves three are purple and yellow with lines and one is solid purple',\n",
    " 'the pretty flower has dark and white petals on it',\n",
    " 'this flower has petals that are white with yellow stamen'\n",
    "]\n",
    "\n",
    "sample_sentences_ids = []\n",
    "for sample_sentence in sample_sentences:\n",
    "    sample_sentence_id = tokenizer.encode(sample_sentence)\n",
    "    phrase = sample_sentence_id + [49407] * (77 - len(sample_sentence_id))\n",
    "    sample_sentences_ids.append(phrase)\n",
    "    \n",
    "sample_sentences_ids = np.array(sample_sentences_ids)\n",
    "sample_sentences_emb = text_encoder([sample_sentences_ids, test_pos_ids])\n",
    "\n",
    "un_sample_sentences_emb = text_encoder([test_uncondition_caps, test_pos_ids])\n",
    "\n",
    "train_sample_sentences = [\n",
    " 'the flower has bright purple petals and its pistils are dark purple',\n",
    " 'this flower has petals that are yellow and very stingy',\n",
    " 'this flower has layers of light yellow sepals holding larger layers of bright pink petals',\n",
    " 'the flower petals are rounded in shape and are bright yellow in clor',\n",
    " 'this flower is bright yellow in color and has petals that are very skinny and long',\n",
    " 'this flower has a lower row of pointed white petals and an upper row of long thin purple petals',\n",
    " 'this flower has petals that are purple with yellow stame',\n",
    " 'this flower is pink and white in color with petals that are very small',\n",
    " 'this flower is white in color and has petals that are ruffled',\n",
    " 'a flower with a singular conical pink petal with black stripes and orange dotting on the interior of the cone'\n",
    "]\n",
    "\n",
    "train_sample_sentences_ids = []\n",
    "for sample_sentence in train_sample_sentences:\n",
    "    sample_sentence_id = tokenizer.encode(sample_sentence)\n",
    "    phrase = sample_sentence_id + [49407] * (77 - len(sample_sentence_id))\n",
    "    train_sample_sentences_ids.append(phrase)\n",
    "    \n",
    "train_sample_sentences_ids = np.array(train_sample_sentences_ids)\n",
    "train_sample_sentences_emb = text_encoder([train_sample_sentences_ids, test_pos_ids])\n",
    "\n",
    "print(sample_sentences_emb.shape)\n",
    "print(train_sample_sentences_emb.shape)\n",
    "print(un_sample_sentences_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(self, image_size, WIDTHS, BLOCK_DEPTH, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalizer = layers.Normalization()\n",
    "        self.network = get_network(image_size, WIDTHS, BLOCK_DEPTH) #image_size, WIDTHS, BLOCK_DEPTH\n",
    "        self.EMA_network = keras.models.clone_model(self.network)\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
    "        self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.noise_loss_tracker, self.image_loss_tracker]\n",
    "\n",
    "    def denormalize(self, images):\n",
    "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(images, 0.0, 1.0)\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # diffusion times -> angles\n",
    "        start_angle = tf.acos(MAX_SIGNAL_RATE)\n",
    "        end_angle = tf.acos(MIN_SIGNAL_RATE)\n",
    "\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    @tf.function\n",
    "    def denoise(self, noisy_images, caption, noise_rates, signal_rates, training):\n",
    "        print(\"In denoise\")\n",
    "        if training:\n",
    "            network = self.network\n",
    "        else:\n",
    "            network = self.EMA_network\n",
    "        pred_noises = network([noisy_images, noise_rates**2, caption], training=training)\n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps, caption_emb, un_caption_emb, cfg_scale):\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        next_noisy_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_images = next_noisy_images\n",
    "\n",
    "            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            \n",
    "            un_pred_noises, un_pred_images = self.denoise(\n",
    "                noisy_images, un_caption_emb, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, caption_emb, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            \n",
    "            pred_noises = un_pred_noises + cfg_scale *(pred_noises-un_pred_noises)\n",
    "            pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_noisy_images = (\n",
    "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
    "            )\n",
    "\n",
    "\n",
    "        return pred_images\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps, caption_emb, un_caption_emb, cfg_scale, seed=None):\n",
    "        initial_noise = tf.random.normal(shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, 3), seed=seed)\n",
    "        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps, caption_emb, un_caption_emb, cfg_scale)\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        return generated_images\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, images, cap):\n",
    "        print(\"Tracing...(This line should be printed only once if @tf.function is enabled.)\")\n",
    "        images = self.normalizer(images, training=True)\n",
    "        noises = tf.random.normal(shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(BATCH_SIZE, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, cap, noise_rates, signal_rates, training=True\n",
    "            )\n",
    "\n",
    "            noise_loss = self.loss(noises, pred_noises)  # used for training\n",
    "            image_loss = self.loss(images, pred_images)  # only used as metric\n",
    "\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "        self.image_loss_tracker.update_state(image_loss)\n",
    "\n",
    "        if global_steps < START_EMA:\n",
    "            for weight, EMA_weight in zip(self.network.weights, self.EMA_network.weights):\n",
    "                EMA_weight.assign(weight)            \n",
    "        else:\n",
    "            for weight, EMA_weight in zip(self.network.weights, self.EMA_network.weights):\n",
    "                EMA_weight.assign(EMA * EMA_weight + (1 - EMA) * weight)\n",
    "            \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, idx, caps, cfg_scale=7.5, un_caps=uncondition_caps_emb, batch_size=BATCH_SIZE, seed=None):\n",
    "        generated_images = self.generate(\n",
    "            num_images=batch_size,\n",
    "            diffusion_steps=PLOT_DIFFUSION_STEPS,\n",
    "            caption_emb=caps,\n",
    "            un_caption_emb=un_caps, \n",
    "            cfg_scale=cfg_scale, \n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            img = tf.image.resize(generated_images[i], size=[64, 64], antialias=True)\n",
    "            img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "            plt.imsave(INFERENCE_DIR+'/inference_{:04d}.jpg'.format(idx[i].numpy()), img.numpy())\n",
    "        return\n",
    "    \n",
    "    def merge(self, images, size):\n",
    "        h, w = images.shape[1], images.shape[2]\n",
    "        img = np.zeros((h * size[0], w * size[1], 3))\n",
    "        for idx, image in enumerate(images):\n",
    "            i = idx % size[1]\n",
    "            j = idx // size[1]\n",
    "            img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "        return img\n",
    "\n",
    "    def imsave(self, images, size, path):\n",
    "        return plt.imsave(path, self.merge(images, size))\n",
    "\n",
    "    def save_images(self, images, size, image_path):\n",
    "        return self.imsave(images, size, image_path)\n",
    "\n",
    "    def plot_images(self, epoch=None, cfg_scale=7.5, caps=sample_sentences_emb, un_caps=un_sample_sentences_emb, num_rows=2, num_cols=5, img_name='/train_'):\n",
    "        generated_images = self.generate(\n",
    "            num_images=num_rows * num_cols,\n",
    "            diffusion_steps=PLOT_DIFFUSION_STEPS,\n",
    "            caption_emb=caps,\n",
    "            un_caption_emb=un_caps, \n",
    "            cfg_scale=cfg_scale, \n",
    "            seed=87\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                index = row * num_cols + col\n",
    "                plt.subplot(num_rows, num_cols, index + 1)\n",
    "                plt.imshow(generated_images[index])\n",
    "                plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        self.save_images(generated_images, [num_rows, num_cols], SAMPLE_DIR + img_name + '{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x):\n",
    "    embedding_min_frequency = 1.0\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(embedding_min_frequency),\n",
    "            tf.math.log(EMBEDDING_MAX_FREQUENCY),\n",
    "            EMBEDDING_DIMENSIONS // 2,\n",
    "        )\n",
    "    )\n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def td_dot(a, b):\n",
    "    aa = tf.reshape(a, (-1, a.shape[2], a.shape[3]))\n",
    "    bb = tf.reshape(b, (-1, b.shape[2], b.shape[3]))\n",
    "    cc = keras.backend.batch_dot(aa, bb)\n",
    "    return tf.reshape(cc, (-1, a.shape[1], cc.shape[1], cc.shape[2]))\n",
    "\n",
    "def TransformerBlock(dim, n_heads, d_head):\n",
    "    multi_head_attention = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_head)\n",
    "\n",
    "    def feed_forward_network(x):\n",
    "        xp = layers.Dense(dim * 2)(x)\n",
    "        x, gate = xp[..., :dim], xp[..., dim:]  \n",
    "        return x + gate\n",
    "\n",
    "    def apply(x, context=None):\n",
    "        # Store the original shape of x\n",
    "        original_shape = tf.shape(x)\n",
    "        batch_size, seq_length, features = original_shape[0], original_shape[1], original_shape[2]\n",
    "\n",
    "        # Layer normalization and reshaping to rank 3\n",
    "        x_norm = layers.LayerNormalization(epsilon=1e-5)(x)\n",
    "        x_norm = tf.reshape(x_norm, [batch_size, -1, features])\n",
    "\n",
    "        # Prepare context if provided\n",
    "        if context is not None:\n",
    "            context = layers.LayerNormalization(epsilon=1e-5)(context)\n",
    "            context = tf.reshape(context, [batch_size, -1, context.shape[-1]])  # Reshape to rank 3\n",
    "\n",
    "        # Apply MultiHeadAttention\n",
    "        if context is not None:\n",
    "            attention_output = multi_head_attention(x_norm, context)\n",
    "        else:\n",
    "            attention_output = multi_head_attention(x_norm, x_norm)\n",
    "\n",
    "        # Reshape attention_output to match the original shape of x\n",
    "        attention_output = tf.reshape(attention_output, original_shape)\n",
    "\n",
    "        # Add the attention output to the original x\n",
    "        x = attention_output + x\n",
    "\n",
    "        # Apply feedforward network\n",
    "        x = layers.Dense(dim)(feed_forward_network(layers.LayerNormalization(epsilon=1e-5)(x))) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n",
    "        )(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "def DownBlock(width, BLOCK_DEPTH):\n",
    "    def apply(x):\n",
    "        x, t, content, skips = x\n",
    "        tt = layers.UpSampling2D(size=int(x.shape[1]/t.shape[1]), interpolation=\"nearest\")(t)\n",
    "        x = ResidualBlock(width)(layers.Concatenate()([x, tt]))\n",
    "        skips.append(x)\n",
    "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "\n",
    "        for _ in range(BLOCK_DEPTH):\n",
    "            tt = layers.UpSampling2D(size=int(x.shape[1]/t.shape[1]), interpolation=\"nearest\")(t)\n",
    "            x = ResidualBlock(width)(layers.Concatenate()([x, tt]))\n",
    "\n",
    "            # TransformerBlock\n",
    "            x = TransformerBlock(width, HEAD, int(width/HEAD))(x, context=content)\n",
    "\n",
    "            skips.append(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(width, BLOCK_DEPTH):\n",
    "    def apply(x):\n",
    "        x, t, content, skips = x\n",
    "        for _ in range(BLOCK_DEPTH):\n",
    "            tt = layers.UpSampling2D(size=int(x.shape[1]/t.shape[1]), interpolation=\"nearest\")(t)\n",
    "            s_pop = skips.pop()\n",
    "            x = layers.Concatenate()([x, s_pop, tt])\n",
    "            x = ResidualBlock(width)(x)\n",
    "\n",
    "            # TransformerBlock\n",
    "            x = TransformerBlock(width, HEAD, int(width/HEAD))(x, context=content)\n",
    "\n",
    "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        tt = layers.UpSampling2D(size=int(x.shape[1]/t.shape[1]), interpolation=\"nearest\")(t)\n",
    "        s_pop = skips.pop()\n",
    "        x = layers.Concatenate()([x, s_pop, tt])\n",
    "        x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "def get_network(image_size, WIDTHS, BLOCK_DEPTH):\n",
    "    noisy_images = layers.Input(shape=(image_size, image_size, 3))\n",
    "    noise_variances = layers.Input(shape=(1, 1, 1))\n",
    "    caption = layers.Input(shape=(77, 768))\n",
    "\n",
    "    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
    "    c = caption\n",
    "    x = layers.Conv2D(32, kernel_size=1)(noisy_images)\n",
    "\n",
    "    skips = []\n",
    "    for width in WIDTHS[:-1]:\n",
    "        x = DownBlock(width, BLOCK_DEPTH)([x, e, c, skips])\n",
    "\n",
    "    for _ in range(BLOCK_DEPTH):\n",
    "        t = layers.UpSampling2D(size=int(x.shape[1]/e.shape[1]), interpolation=\"nearest\")(e)\n",
    "        x = ResidualBlock(WIDTHS[-1])(layers.Concatenate()([x, t]))\n",
    "\n",
    "    for width in reversed(WIDTHS[:-1]):\n",
    "        x = UpBlock(width, BLOCK_DEPTH)([x, e, c, skips])\n",
    "\n",
    "    x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "\n",
    "    return keras.Model([noisy_images, noise_variances, caption], x, name=\"residual_unet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf35e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network = get_network(IMAGE_SIZE, WIDTHS, BLOCK_DEPTH)\n",
    "# network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaee91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create and compile the model\n",
    "model = DiffusionModel(IMAGE_SIZE, WIDTHS, BLOCK_DEPTH)\n",
    "\n",
    "cosine_decay_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = 1e-6,\n",
    "    decay_steps = 1, \n",
    "    warmup_steps = 10000, \n",
    "    warmup_target = 1e-4,\n",
    "    alpha=1.0,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(\n",
    "        learning_rate=cosine_decay_scheduler,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    ),\n",
    "    loss=tf.keras.losses.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eae76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train_dataset)\n",
    "total_steps = steps_per_epoch*NUM_EPOCHS\n",
    "\n",
    "# ckp = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
    "ckp = CHECKPOINT_DIR + '/ddpm-71'\n",
    "\n",
    "if ckp:\n",
    "    init_epoch = (int(ckp.split('-')[-1]))+1\n",
    "    init_step = steps_per_epoch*(init_epoch-1)+1\n",
    "    global_steps = tf.Variable(init_step, trainable=False, dtype=tf.int64)\n",
    "    ckpt = tf.train.Checkpoint(epoch=tf.Variable(init_epoch),\n",
    "                               step=tf.Variable(init_step), \n",
    "                               net=model)\n",
    "    ckpt.restore(ckp)\n",
    "    print(f'Resume training from global_epoch {init_epoch-1}, global_steps {init_step-1}')\n",
    "else:\n",
    "    init_epoch = 1\n",
    "    init_step = 1\n",
    "    global_steps = tf.Variable(init_step, trainable=False, dtype=tf.int64)\n",
    "    ckpt = tf.train.Checkpoint(epoch=tf.Variable(0), \n",
    "                               step=tf.Variable(0), \n",
    "                               net=model)\n",
    "    print(f'Start training from global_epoch {init_epoch}, global_steps {init_step}')\n",
    "\n",
    "manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=400, \n",
    "                                     checkpoint_name='ddpm')\n",
    "\n",
    "\n",
    "feature_ds = train_dataset.map(lambda x, y: x)\n",
    "model.normalizer.adapt(feature_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d0aa4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.plot_images(0)\n",
    "for epoch in range(init_epoch, NUM_EPOCHS+1):\n",
    "    print(f'Epoch {epoch:>2}/{NUM_EPOCHS}')\n",
    "    ckpt.epoch.assign_add(1)\n",
    "\n",
    "    for image_data, cap in train_dataset:\n",
    "        if np.random.random() < 0.1:\n",
    "            cap_emb = uncondition_caps_emb\n",
    "        else:\n",
    "            cap_emb = text_encoder([cap, pos_ids])\n",
    "            \n",
    "        matrix = model.train_step(image_data, cap_emb)\n",
    "          \n",
    "        global_steps.assign_add(1)\n",
    "        \n",
    "        print(\"=> STEP %d/%d  lr: %f n_loss: %f  i_loss: %f\" % (global_steps, total_steps, model.optimizer.lr, matrix['n_loss'].numpy(), matrix['i_loss'].numpy()), end='\\r')\n",
    "    \n",
    "    print()\n",
    "    save_path = manager.save()\n",
    "    if save_path:\n",
    "        print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.epoch), save_path)) \n",
    "        \n",
    "    if epoch%5==0:\n",
    "        model.plot_images(epoch, cfg_scale=3.6)\n",
    "        model.plot_images(epoch, cfg_scale=3.6, caps=train_sample_sentences_emb, img_name='/train_ex_')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6435c21",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(filenames, batch_size, data_generator):\n",
    "    data = pd.read_pickle(filenames)\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        img_raw_caption_id = captions[i]\n",
    "        img_raw_words = [id2word_dict[str(w_id.astype(np.int32))] for w_id in img_raw_caption_id if w_id != '5427']\n",
    "        img_raw_caption = \" \".join(img_raw_words)\n",
    "        img_caption_id = tokenizer.encode(img_raw_caption)\n",
    "        phrase = img_caption_id + [49407] * (MAX_TEXT_LENGTH - len(img_caption_id)) \n",
    "        assert(len(phrase) == MAX_TEXT_LENGTH)\n",
    "        caption.append(phrase)\n",
    "    assert len(captions) == len(caption)\n",
    "        \n",
    "    caption = np.asarray(caption).astype(np.int32)\n",
    "    index = np.asarray(data['ID'].values)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ceb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_generator('./dataset/testData.pkl', BATCH_SIZE, testing_data_generator)\n",
    "\n",
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c403ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def inference(dataset, cfg_scale, seed=None):\n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        print(f\"=> {step}/{EPOCH_TEST}\", end='\\r')\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "            \n",
    "        cap_emb = text_encoder([captions, pos_ids])\n",
    "        model.test_step(idx, cap_emb, cfg_scale=cfg_scale, seed=seed)\n",
    "        step += 1\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fc7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In denoise\n",
      "Time for inference is 59.1357 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset, 3.6, 900523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n",
      "--------------Evaluation Success-----------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('./testing')\n",
    "!python inception_score.py ../inference/final_version ../final_version.csv 39\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec505e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.440848\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "score_path = './final_version.csv'\n",
    "\n",
    "if os.path.exists(score_path):\n",
    "    df_score = pd.read_csv(score_path)\n",
    "    mean_score = np.mean(df_score['score'].values)\n",
    "    print(f'Mean Score: {mean_score:f}')\n",
    "else:\n",
    "    print('Evaluation Failed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
