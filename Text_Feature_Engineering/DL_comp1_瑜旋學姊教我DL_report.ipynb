{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 2023 DataLab Cup1 : Predicting News Popularity(Text Feature Engineering)\n","##### Competition for CS565600 Deep Learning\n","* 組別: 瑜旋學姊教我DL\n","* 成員: 112062531 王興彥 112062559 邱仁緯 112062632 林沁璿\n","* Public: 0.59617\n","* Private: 0.59894"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":12207,"status":"ok","timestamp":1697251584146,"user":{"displayName":"XingYan Wang","userId":"10290026631758581261"},"user_tz":-480},"id":"fRe0-htSLlD3"},"outputs":[],"source":["import os\n","import warnings\n","import pandas as pd\n","\n","%matplotlib inline\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","if not os.path.exists(\"output/\") : os.mkdir(\"output/\")\n","\n","df_train = pd.read_csv('csv/train.csv')\n","df_test = pd.read_csv('csv/test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing\n","1. 使用BeautifulSoup讀取文字用來parser html格式，方便存取feature。\n","2. 將能夠獲取且可能對預測造成影響的feature找出。\n","    * title: 文章標題。\n","    * title_len: 標題長度。\n","    * topic: 文章主題。\n","    * date_time: 文章發表時間，使用dateutil的parser將各項時間取出，因為有部分資料找不到時間資訊。\n","        * 解決方式: 找到與缺乏時間資訊的資料相同的topic和popularity的資料，並將其時間資訊拿來當default值。\n","    * content_len: 文章長度。\n","    "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697251584146,"user":{"displayName":"XingYan Wang","userId":"10290026631758581261"},"user_tz":-480},"id":"rIhmRQ4Cinrl"},"outputs":[],"source":["import re\n","from bs4 import BeautifulSoup\n","from dateutil import parser\n","\n","def preprocessor(text):\n","    \n","    text = BeautifulSoup(text, 'html.parser')\n","    \n","    title = text.body.h1.string.lower().strip()\n","    title_len = len(title)   \n","    \n","    l_list = text.body.find('footer', {'class': 'article-topics'}).find_all('a')\n","    topic_list = [a.string.strip().lower() for a in l_list]\n","    topic = ' '.join([re.sub('\\s+', '_', t) for t in topic_list])\n","    \n","    article_info = text.head.find('div', {'class': 'article-info'})\n","    try :\n","        date_time = article_info.time['datetime']\n","    except:\n","        date_time = 'Fri, 15 Feb 2013 15:49:08'\n","\n","    parse_time = parser.parse(date_time)\n","    year = parse_time.year\n","    month = parse_time.month\n","    day = parse_time.day\n","    weekday = parse_time.isoweekday()\n","    hour = parse_time.hour\n","    minute = parse_time.minute\n","    second = parse_time.second\n","\n","    content = text.body.find('section', {'class': 'article-content'}).get_text()\n","    content_len = len(content)\n","\n","    return title, title_len, topic, day, month, year, weekday, hour, minute, second, content_len"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["feature_train = []\n","feature_test = []\n","\n","for text in df_train['Page content']:\n","    feature_train.append(preprocessor(text))\n","for text in df_test['Page content']:\n","    feature_test.append(preprocessor(text))\n","\n","df_train_new = pd.DataFrame(feature_train,\n","    columns=['Title', 'Title_Len', 'Topic', 'Day', 'Month', 'Year',\n","             'Weekday', 'Hour', 'Minute', 'Second', 'Content_Len']\n",")\n","df_test_new = pd.DataFrame(feature_test,\n","    columns=['Title', 'Title_Len', 'Topic', 'Day', 'Month', 'Year',\n","             'Weekday', 'Hour', 'Minute', 'Second', 'Content_Len']\n",")\n","\n","df_train_new.to_csv('csv/df_train_new.csv', index=False)\n","df_test_new.to_csv('csv/df_test_new.csv', index=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2364,"status":"ok","timestamp":1697251586507,"user":{"displayName":"XingYan Wang","userId":"10290026631758581261"},"user_tz":-480},"id":"056h7ntX1_HP"},"outputs":[],"source":["df_train_new = pd.read_csv('csv/df_train_new.csv')\n","df_test_new = pd.read_csv('csv/df_test_new.csv')"]},{"cell_type":"markdown","metadata":{},"source":["3. 將缺資訊的資料補上default\n","    * Topic: 補上None。\n","    * Content_Len: 直接從資料中找出原文，只有一個，而實際查看後大約100字左右。"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df_train_sel = df_train_new.drop(columns=['Title', 'Minute', 'Second'])\n","df_test_sel = df_test_new.drop(columns=['Title', 'Minute', 'Second'])\n","\n","\n","df_train_sel['Topic'] = df_train_sel['Topic'].where(pd.notnull(df_train_sel['Topic']), 'None')\n","df_train_sel['Content_Len'] = df_train_sel['Content_Len'].where(pd.notnull(df_train_sel['Content_Len']), 100)\n","df_test_sel['Topic'] = df_test_sel['Topic'].where(pd.notnull(df_test_sel['Topic']), 'None')\n","df_test_sel['Content_Len'] = df_test_sel['Content_Len'].where(pd.notnull(df_test_sel['Content_Len']), 100)"]},{"cell_type":"markdown","metadata":{},"source":["4. 透過Natural Language Toolkit(NLTK)對Topic進行Word Stemming。"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2068,"status":"ok","timestamp":1697251588573,"user":{"displayName":"XingYan Wang","userId":"10290026631758581261"},"user_tz":-480},"id":"VH3GsNwj3jxJ","outputId":"a84f03fa-478a-48a3-a1fc-04bdb25afd25"},"outputs":[],"source":["import numpy as np\n","from nltk.stem.porter import PorterStemmer\n","\n","def tokenizer_stem(text):\n","    if type(text) == np.ndarray:\n","        text = text[0]\n","    porter = PorterStemmer()\n","    return [porter.stem(word) for word in re.split('\\s+', text.strip())]"]},{"cell_type":"markdown","metadata":{},"source":["5. 使用ColumnTransformer，針對Topic進行轉換，方便後續pipeline使用。\n","\n","    <https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html>"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697251588573,"user":{"displayName":"XingYan Wang","userId":"10290026631758581261"},"user_tz":-480},"id":"xbGO-tbF5RcH"},"outputs":[],"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","trans = ColumnTransformer(\n","    [('Topic', CountVectorizer(tokenizer=tokenizer_stem, lowercase=False), [1])], n_jobs=-1, remainder='passthrough')"]},{"cell_type":"markdown","metadata":{},"source":["6. 將訓練資料切成training set和validation set，使用0.3的ratio進行切割。"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1697251588573,"user":{"displayName":"XingYan Wang","userId":"10290026631758581261"},"user_tz":-480},"id":"lt9vjLq55wzV","outputId":"5affac37-b5cc-44e2-f37f-92189516de7e"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X = df_train_sel.values\n","y = (df_train['Popularity'].values == 1).astype(int)\n","\n","X_test = df_test_sel.values\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1697251589591,"user":{"displayName":"XingYan Wang","userId":"10290026631758581261"},"user_tz":-480},"id":"uOVXMl1L6hW3"},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","from sklearn.metrics import roc_auc_score\n","\n","def training(clf):\n","    cv = cross_validate(clf, X, y, scoring='roc_auc', return_train_score=True, return_estimator=True)\n","    print('train score: {:.5f}'.format(np.mean(cv['train_score'])))\n","    print('valid score: {:.5f}'.format(np.mean(cv['test_score'])))\n","\n","    clf.fit(X_train, y_train)\n","    print('train score: {:.5f}'.format(roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])))\n","    print('valid score: {:.5f}'.format(roc_auc_score(y_valid, clf.predict_proba(X_valid)[:, 1])))\n","    \n","    return clf"]},{"cell_type":"markdown","metadata":{},"source":["## Model\n","\n","1. 使用多種classifier(RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, <br/>XGBClassifier, CatBoostClassifier, LGBMClassifier, VotingClassifier)進行比對挑選出最好的結果，最後選擇LGBMClassifier。\n","\n","2. 參數的部分有透過GridSearchCV搜尋，但效果不理想所以手動挑選learning_rate以及n_estimators。"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train score: 0.67383\n","valid score: 0.60283\n","train score: 0.68009\n","valid score: 0.59993\n"]}],"source":["from sklearn.pipeline import Pipeline\n","from lightgbm import LGBMClassifier\n","\n","lgbm = Pipeline([('ct', trans),\n","                 ('clf', LGBMClassifier( n_jobs=-1, verbose=0, random_state=0, learning_rate=0.01, n_estimators=270))])\n","lgbm = training(lgbm)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["y_score = lgbm.predict_proba(X_test)[:, 1]\n","df_pred = pd.DataFrame({'Id': df_test['Id'], 'Popularity': y_score})\n","df_pred.to_csv('output/test_pred.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion\n","\n","這次Text Feature Engineering學習到很多東西，從基本的資料處理到一些library的使用，其中最大的收穫就是理解到老師一開始講的話，好的資料對model的影響是非常大的，所以資料前處理在ML是相當重要的部分，這點在挑選feature和feature extraction的時候能夠感受出很明顯的差異，資料不是越多越好，選出具有代表性的feature和處理出好的feature才是重點。\n","\n","除了上面提到的部分還有一點可以檢討的就是在比賽過程中太執著於public score，雖然最後private score也表現得更好，但是因為執著於public score導致沒選出最好的結果0.60232。"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPkPey5tddsR3uAUWd3F82f","gpuType":"T4","mount_file_id":"1kNpNzc9MpoW4E1lytHCIBsjvlTAYa4Gd","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
