{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation.environment import TrainingEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official hyperparameters for this competition (do not modify)\n",
    "N_TRAIN_USERS = 1000\n",
    "N_TEST_USERS = 2000\n",
    "N_ITEMS = 209527\n",
    "HORIZON = 2000\n",
    "TEST_EPISODES = 5\n",
    "SLATE_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "USER_DATA = os.path.join('dataset', 'user_data.json')\n",
    "ITEM_DATA = os.path.join('dataset', 'item_data.json')\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_PATH = os.path.join('output', 'output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_USERS = 2000\n",
    "N_ITEMS = 209527\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ratings = pd.DataFrame(columns=['user_id', 'item_id', 'rating'])\n",
    "df_ratings = pd.read_csv('./dataset/ratings.csv')\n",
    "df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunkSVDRecommender(tf.keras.Model):\n",
    "    '''\n",
    "    Simplified Funk-SVD recommender model\n",
    "    '''\n",
    "    def __init__(self, m_users: int, n_items: int, embedding_size: int, learning_rate: float):\n",
    "        '''\n",
    "        Constructor of the model\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.m = m_users\n",
    "        self.n = n_items\n",
    "        self.k = embedding_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # user embeddings P\n",
    "        self.P = tf.Variable(tf.keras.initializers.RandomNormal()(shape=(self.m, self.k)))\n",
    "\n",
    "        # item embeddings Q\n",
    "        self.Q = tf.Variable(tf.keras.initializers.RandomNormal()(shape=(self.n, self.k)))\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, user_ids: tf.Tensor, item_ids: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Forward pass used in training and validating\n",
    "        '''\n",
    "        # dot product the user and item embeddings corresponding to the observed interaction pairs to produce predictions\n",
    "        y_pred = tf.reduce_sum(tf.gather(self.P, indices=user_ids) * tf.gather(self.Q, indices=item_ids), axis=1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    @tf.function\n",
    "    def compute_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Compute the MSE loss of the model\n",
    "        '''\n",
    "        loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Train the model with one batch\n",
    "        data: batched user-item interactions\n",
    "        each record in data is in the format ['user_id', 'item_id', 'rating']\n",
    "        '''\n",
    "        user_ids = tf.cast(data[:, 0], dtype=tf.int32)\n",
    "        item_ids = tf.cast(data[:, 1], dtype=tf.int32)\n",
    "        y_true = tf.cast(data[:, 2], dtype=tf.float32)\n",
    "\n",
    "        # slate = self.eval_predict_onestep(user_ids)\n",
    "        # print(slate)\n",
    "        # compute loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(user_ids, item_ids)\n",
    "            loss = self.compute_loss(y_true, y_pred)\n",
    "\n",
    "        # compute gradients\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "        # update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(self, data: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Validate the model with one batch\n",
    "        data: batched user-item interactions\n",
    "        each record in data is in the format ['user_id', 'item_id', 'rating']\n",
    "        '''\n",
    "        user_ids = tf.cast(data[:, 0], dtype=tf.int32)\n",
    "        item_ids = tf.cast(data[:, 1], dtype=tf.int32)\n",
    "        y_true = tf.cast(data[:, 2], dtype=tf.float32)\n",
    "\n",
    "        # compute loss\n",
    "        y_pred = self(user_ids, item_ids)\n",
    "        loss = self.compute_loss(y_true, y_pred)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def eval_predict_onestep(self, query: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Retrieve and return the MovieIDs of the 5 recommended movies given a query\n",
    "        You should return a tf.Tensor with shape=(5,)\n",
    "        query will be a tf.Tensor with shape=(1,) and dtype=tf.int64\n",
    "        query is the UserID of the query\n",
    "        '''\n",
    "        # dot product the selected user and all item embeddings to produce predictions\n",
    "        user_id = tf.cast(query, tf.int32)\n",
    "        y_pred = tf.reduce_sum(tf.gather(self.P, user_id) * self.Q, axis=1)\n",
    "\n",
    "        # select the top 5 items with highest scores in y_pred\n",
    "        y_top_5 = tf.math.top_k(y_pred, k=5).indices\n",
    "\n",
    "        return y_top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EMBEDDING_SIZE = 512\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FunkSVDRecommender(m_users=N_TRAIN_USERS, n_items=N_ITEMS, embedding_size=EMBEDDING_SIZE, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(epoch=tf.Variable(0), net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, './ckpts/FSVD', max_to_keep=10, checkpoint_name='fsvd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the testing environment\n",
    "test_env = TrainingEnvironment()\n",
    "pre = df_ratings.shape[0]\n",
    "cur = 0\n",
    "\n",
    "# The item_ids here is for the random recommender\n",
    "item_ids = [i for i in range(N_ITEMS)]\n",
    "\n",
    "# Repeat the testing process for 5 times\n",
    "for _ in range(100):\n",
    "    # [TODO] Load your model weights here (in the beginning of each testing episode)\n",
    "    # [TODO] Code for loading your model weights...\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "\n",
    "    # Start the testing process\n",
    "    with tqdm(desc='Testing') as pbar:\n",
    "        # Run as long as there exist some active users\n",
    "        while test_env.has_next_state():\n",
    "            # Get the current user id\n",
    "            cur_user = test_env.get_state()\n",
    "            \n",
    "            # [TODO] Employ your recommendation policy to generate a slate of 5 distinct items\n",
    "            # [TODO] Code for generating the recommended slate...\n",
    "            # Here we provide a simple random implementation\n",
    "            slate = model.eval_predict_onestep(cur_user)\n",
    "\n",
    "            # Get the response of the slate from the environment\n",
    "            clicked_id, in_environment = test_env.get_response(slate)\n",
    "\n",
    "            # [TODO] Update your model here (optional)\n",
    "            # [TODO] You can update your model at each step, or perform a batched update after some interval\n",
    "            # [TODO] Code for updating your model...\n",
    "            ratings_data = []\n",
    "            # mask = (df_ratings['user_id'] == cur_user) & (df_ratings['item_id'] == clicked_id)\n",
    "            if clicked_id != -1: # and not mask.empty:\n",
    "                # print('click', cur_user, clicked_id)\n",
    "                ratings_data.append({'user_id': cur_user, 'item_id': clicked_id, 'rating': 5.0})\n",
    "\n",
    "            df = pd.DataFrame(ratings_data)\n",
    "            df_ratings = pd.concat([df_ratings, df], ignore_index=True)\n",
    "            \n",
    "            # Update the progress indicator\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Reset the testing environment\n",
    "    test_env.reset()\n",
    "\n",
    "    # [TODO] Delete or reset your model weights here (in the end of each testing episode)\n",
    "    # [TODO] Code for deleting your model weights...\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    print(df_ratings.shape[0])\n",
    "    df_ratings = df_ratings.drop_duplicates(subset=['user_id', 'item_id'], keep='last', ignore_index=True)\n",
    "\n",
    "# Generate a DataFrame to output the result in a .csv file\n",
    "    cur = df_ratings.shape[0]\n",
    "    print(pre, cur)\n",
    "    pre = cur\n",
    "# df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.sort_values(by=['user_id', 'item_id'])\n",
    "df_ratings\n",
    "df_ratings.to_csv('./dataset/ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_5_star_ratings = len(df_ratings[df_ratings['rating'] == 5.0])\n",
    "print(\"Number of rows with rating = 5.0:\", count_5_star_ratings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
