{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 2023 DataLab Cup2 : CNN for Object Detection\n","##### Competition for CS565600 Deep Learning\n","* 組別: 瑜旋學姊教我DL\n","* 成員: 112062531 王興彥 112062559 邱仁緯 112062632 林沁璿\n","* Public: 0.36306\n","* Private: 0.39025"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tGMUETL5y5JF"},"outputs":[],"source":["classes_name =  [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n","                 \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n","                 \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n","                 \"sheep\", \"sofa\", \"train\",\"tvmonitor\"]"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"B6UEOUPZy5JI"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 Physical GPUs, 1 Logical GPUs\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","\n","training_data_file = open(\"./dataset/pascal_voc_training_data.txt\", \"r\")\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        # Select GPU number 1\n","        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8vxtKrtpy5JI"},"outputs":[],"source":["# common params\n","IMAGE_SIZE = 448\n","BATCH_SIZE = 32\n","NUM_CLASSES = 20\n","MAX_OBJECTS_PER_IMAGE = 20\n","\n","# dataset params\n","DATA_PATH = './dataset/pascal_voc_training_data.txt'\n","IMAGE_DIR = './dataset/VOCdevkit_train/VOC2007/JPEGImages/'\n","\n","# model params\n","CELL_SIZE = 7\n","BOXES_PER_CELL = 2\n","OBJECT_SCALE = 1\n","NOOBJECT_SCALE = 0.5\n","CLASS_SCALE = 1\n","COORD_SCALE = 5\n","\n","# training params\n","LEARNING_RATE = 1e-5\n","EPOCHS = 1\n","\n","# Augmentation dataset params\n","AUG_DATASET = False\n","DATA_AUG_PATH = './dataset/pascal_voc_training_data_aug.txt'\n","IMAGE_AUG_DIR = './dataset/VOCdevkit_train/VOC2007/JPEGImages_Aug/'"]},{"cell_type":"markdown","metadata":{"id":"H7jJj4Way5JJ"},"source":["### Data Augmentation"]},{"cell_type":"markdown","metadata":{},"source":["1. Dealing with Data Imbalance(Over-sampling)\n","    * 由於不同class的資料量非常不平衡，所以必須把class之間的資料量做平衡，平衡後對於loss的下降速度以及最終預測結果都會有顯著的提升。\n","    * 這次做資料量平衡的手段是使用Over-sampling，將資料量較少的class增加到一個相對平衡的數量。\n","    * 這邊的方法是參考下方連結，將所有class都增加到至少有3000個資料。\n","    * Reference: https://reurl.cc/q0ANdg"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Define a function to process each line\n","def process_line(line, max_objects):\n","    data = line.strip().split()\n","\n","    # Convert the remaining data to float and limit the number of objects\n","    record = [float(num) for num in data[1:]]\n","    if len(record) > max_objects * 5:\n","        record = record[:max_objects * 5]\n","\n","    return data[0], record, len(record) // 5\n","\n","# Initialize lists\n","image_names = []\n","record_list = []\n","object_num_list = []\n","\n","# Read the file and process each line\n","with open(DATA_PATH, 'r') as file:\n","    for line in file:\n","        name, record, obj_num = process_line(line, MAX_OBJECTS_PER_IMAGE)\n","        image_names.append(name)\n","        record_list.append(record)\n","        object_num_list.append(obj_num)\n","\n","# Convert records to bounding boxes and create a dictionary mapping names to boxes\n","bboxes_list = [np.array(record).reshape((-1, 5)) for record in record_list]\n","name_to_bboxes_dict = dict(zip(image_names, bboxes_list))\n","\n","# Extract object classes\n","object_class_list = [[int(record[i]) for i in range(4, len(record), 5)] for record in record_list]\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 0) aeroplane    331\t 1) bicycle      412\t 2) bird         577\t 3) boat         398\n"," 4) bottle       612\t 5) bus          271\t 6) car         1634\t 7) cat          389\n"," 8) chair       1423\t 9) cow          356\t10) diningtable  309\t11) dog          536\n","12) horse        403\t13) motorbike    387\t14) person      5318\t15) pottedplant  603\n","16) sheep        353\t17) sofa         419\t18) train        328\t19) tvmonitor    366\n"]}],"source":["class_count = [0] * NUM_CLASSES\n","for class_list in object_class_list:\n","    for c in class_list:\n","        class_count[c] += 1\n","\n","# Print class count in a formatted way\n","for i, class_name in enumerate(classes_name):\n","    print(f'{i:2d}) {class_name:11} {class_count[i]:4d}', end=('\\n' if (i + 1) % 4 == 0 else '\\t'))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 0) aeroplane   3001\t 1) bicycle     3000\t 2) bird        3000\t 3) boat        3000\n"," 4) bottle      3000\t 5) bus         3000\t 6) car         3002\t 7) cat         3000\n"," 8) chair       3001\t 9) cow         3002\t10) diningtable 3000\t11) dog         3000\n","12) horse       3000\t13) motorbike   3000\t14) person      5318\t15) pottedplant 3004\n","16) sheep       3007\t17) sofa        3000\t18) train       3000\t19) tvmonitor   3000\n"]}],"source":["# Initialization\n","aug_image_name_list = []\n","class_count = np.zeros(NUM_CLASSES, int)\n","image_count = np.zeros(len(image_names), int)\n","\n","# Generate augmented image names and update class counts\n","for i, class_list in enumerate(object_class_list):\n","    aug_image_name_list.append(f'{image_count[i]}_{image_names[i]}')\n","    image_count[i] += 1\n","    for c in class_list:\n","        class_count[c] += 1\n","\n","#  Set the minimum number of images per class\n","LOWER_BOUND = 3000\n","sorted_indices = np.argsort(object_num_list)[::-1]\n","invalid_aug = np.zeros(len(image_names), int)\n","\n","# Ensure each class has at least LOWER_BOUND number of images\n","while not (class_count >= LOWER_BOUND).all():\n","    for i in sorted_indices:\n","        if invalid_aug[i] or (class_count[object_class_list[i]] >= LOWER_BOUND).any():\n","            invalid_aug[i] = 1\n","            continue\n","\n","        aug_image_name_list.append(f'{image_count[i]}_{image_names[i]}')\n","        image_count[i] += 1\n","        for c in object_class_list[i]:\n","            class_count[c] += 1\n","\n","# Print class count in a formatted way\n","for i, class_name in enumerate(classes_name):\n","    print(f'{i:2d}) {class_name:11} {class_count[i]:4d}', end=('\\n' if (i + 1) % 4 == 0 else '\\t'))"]},{"cell_type":"markdown","metadata":{},"source":["2. Perform Data Augmentation on new data\n","    * 上方透過複製原有的資料達到資料平衡，但若是使用相同的資料作訓練效果會非常有限，所以這邊必須將剛才新增的資料做一些轉換。\n","    * 轉換的方法是透過下方連結提供的方法，除了以下四種方式外還有其他轉換，但其他轉換會產生一些bug，所以只使用了這四種。\n","    * 分別是調整Hue, Saturation, Value(Lightness), Flip, Scale, Shear。\n","    * reference: https://reurl.cc/QZAbLM"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"2-2h_6hKy5JL"},"outputs":[],"source":["from data_aug.data_aug import *\n","\n","def generate_aug_dataset():\n","    f =  open(DATA_AUG_PATH, 'w')\n","    np.random.seed(0)\n","    seq = Sequence([RandomHSV(50, 50, 50),\n","                    RandomHorizontalFlip(0.5),\n","                    RandomScale(0.15),\n","                    RandomShear(0.15)])\n","\n","    for name in aug_image_name_list:\n","        _, img_name = name.split('_')\n","    \n","        img_path = os.path.join(IMAGE_DIR, img_name)\n","        img_file = tf.io.read_file(img_path)\n","        img = tf.io.decode_jpeg(img_file, channels=3)\n","        bboxes = name_to_bboxes_dict.get(img_name)\n","    \n","        if not name.startswith('0'):\n","            img, bboxes = seq(img.numpy().copy(), bboxes.copy())\n","    \n","        encoded_img = tf.io.encode_jpeg(img)\n","        augmented_img_path = os.path.join(IMAGE_AUG_DIR, name)\n","        tf.io.write_file(augmented_img_path, encoded_img)\n","    \n","        bbox_str_list = [str(int(b)) for bbox in bboxes for b in bbox]\n","        bbox_line = ' '.join([name] + bbox_str_list)\n","        f.write(bbox_line + '\\n')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"KuWPj-5ly5JL"},"outputs":[],"source":["if AUG_DATASET:\n","    generate_aug_dataset()"]},{"cell_type":"markdown","metadata":{"id":"3OEL0AE7y5JL"},"source":["3. Apply mosaic data augmentation to raw data\n","    * 參考YoloV4中Mosaic方法，對原本的資料進一步進行data augmentation。\n","    * Mosaic是挑選四張圖片，並對其進行轉換，而除了轉換之外再將這四張圖片隨機拼接，形成新的圖片，具有豐富圖片背景的效果。\n","    * Mosaic除了能夠豐富dataset之外，也夠將增加的資料有效降低，降低data augmentation要付出的訓練時間overhead。\n","    * 這次使用Mosaic增加了與原始dataset一樣數量的資料。\n","    * Reference: https://reurl.cc/7MQkal"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gGwpCmbyy5JL"},"outputs":[],"source":["def find_line_by_name(filename, name):\n","    with open(filename, 'r') as file:\n","        for line in file:\n","            if name in line:\n","                return line\n","    return None"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def convert_line_format(line):\n","    # Split the line by spaces\n","    parts = line.split()\n","    # The first part is the image filename, so we keep it separate\n","    filename = parts[0]\n","    # The rest are assumed to be numeric values for bounding boxes and classes\n","    numeric_parts = parts[1:]\n","    \n","    # Reformat the numeric parts by grouping every five elements\n","    reformatted_numeric_parts = [' '.join([','.join(numeric_parts[i:i+5])]) for i in range(0, len(numeric_parts), 5)]\n","    \n","    # Combine the filename and the reformatted numeric parts\n","    new_line = filename + ' ' + ' '.join(reformatted_numeric_parts)\n","    return new_line"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"i4wwM7n0y5JM"},"outputs":[],"source":["# from utils.random_data import get_random_data_with_Mosaic\n","from myutils.random_data import get_random_data_with_Mosaic\n","from random import sample\n","from PIL import Image\n","\n","input_shape = [IMAGE_SIZE, IMAGE_SIZE]\n","\n","if AUG_DATASET:\n","    img_names = os.listdir(IMAGE_DIR)\n","\n","    for index in range(len(img_names)):\n","        sample_imgs = sample(img_names, 4)\n","        annotation_line = []\n","        for name in sample_imgs:\n","            tmp = find_line_by_name(DATA_PATH, name)\n","            # print(tmp)\n","            if tmp == None: \n","                annotation_line.append(None)\n","                continue\n","                \n","            found_line = IMAGE_DIR + tmp\n","            annotation_line.append(found_line)\n","        \n","        if None in annotation_line:\n","            continue\n","        else:\n","            annotation_line = [convert_line_format(l.strip()) for l in annotation_line]\n","            image_data, box_data = get_random_data_with_Mosaic(annotation_line, input_shape)\n","\n","            img = Image.fromarray(image_data.astype(np.uint8))\n","            img.save(os.path.join(IMAGE_AUG_DIR, 'mosaic_'+str(index).zfill(6)+'.jpg'))\n","            formatted_line = ''\n","            for item in box_data:\n","\n","                int_item = [str(int(num)) for num in item]\n","                my_string = ' '.join(int_item)\n","                formatted_line = formatted_line + ' ' + my_string\n","\n","            formatted_line = 'mosaic_' + '{:06}'.format(index) + '.jpg'+ formatted_line\n","\n","            with open(DATA_AUG_PATH, \"a\") as file:\n","                file.write(formatted_line + \"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["剛開始做Data augmentation時，Over-sampling設定每個class至少要有1000張，並同時做Mosaic augmentation from raw data。\n","\n","在這種情況下進行模型的訓練，發現預測階段時，模型會傾向於預測較多的物件數，儘管圖中只有少量的物件而已。我們認為這是因為Mosaic的比例太高導致，因此將Over-sampling的Lower_bound調高至3000，再重新進行訓練，結果就有明顯的改善。"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Loader"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"aNbL2K4My5JM"},"outputs":[],"source":["class DatasetGenerator:\n","    \"\"\"\n","    Load pascalVOC 2007 dataset and creates an input pipeline.\n","    - Reshapes images into 448 x 448\n","    - converts [0 1] to [-1 1]\n","    - shuffles the input\n","    - builds batches\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.image_names = []\n","        self.record_list = []\n","        self.object_num_list = []\n","        # filling the record_list\n","        input_file = open(DATA_AUG_PATH, 'r')\n","\n","        for line in input_file:\n","            line = line.strip()\n","            ss = line.split(' ')\n","            self.image_names.append(ss[0])\n","\n","            self.record_list.append([float(num) for num in ss[1:]])\n","\n","            self.object_num_list.append(min(len(self.record_list[-1])//5, \n","                                            MAX_OBJECTS_PER_IMAGE))\n","            if len(self.record_list[-1]) < MAX_OBJECTS_PER_IMAGE*5:\n","                # if there are objects less than MAX_OBJECTS_PER_IMAGE, pad the list\n","                self.record_list[-1] = self.record_list[-1] +\\\n","                [0., 0., 0., 0., 0.]*\\\n","                (MAX_OBJECTS_PER_IMAGE-len(self.record_list[-1])//5)\n","                \n","            elif len(self.record_list[-1]) > MAX_OBJECTS_PER_IMAGE*5:\n","               # if there are objects more than MAX_OBJECTS_PER_IMAGE, crop the list\n","                self.record_list[-1] = self.record_list[-1][:MAX_OBJECTS_PER_IMAGE*5]\n","\n","    def _data_preprocess(self, image_name, raw_labels, object_num):\n","        image_file = tf.io.read_file(IMAGE_AUG_DIR+image_name)\n","        image = tf.io.decode_jpeg(image_file, channels=3)\n","\n","        h = tf.shape(image)[0]\n","        w = tf.shape(image)[1]\n","\n","        width_ratio  = IMAGE_SIZE * 1.0 / tf.cast(w, tf.float32) \n","        height_ratio = IMAGE_SIZE * 1.0 / tf.cast(h, tf.float32) \n","\n","        image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n","        image = tf.keras.applications.densenet.preprocess_input(image)\n","        # image = (image/255) * 2 - 1\n","        \n","\n","        raw_labels = tf.cast(tf.reshape(raw_labels, [-1, 5]), tf.float32)\n","\n","        xmin = raw_labels[:, 0]\n","        ymin = raw_labels[:, 1]\n","        xmax = raw_labels[:, 2]\n","        ymax = raw_labels[:, 3]\n","        class_num = raw_labels[:, 4]\n","\n","        xcenter = (xmin + xmax) * 1.0 / 2.0 * width_ratio\n","        ycenter = (ymin + ymax) * 1.0 / 2.0 * height_ratio\n","\n","        box_w = (xmax - xmin) * width_ratio\n","        box_h = (ymax - ymin) * height_ratio\n","\n","        labels = tf.stack([xcenter, ycenter, box_w, box_h, class_num], axis=1)\n","\n","        return image, labels, tf.cast(object_num, tf.int32)\n","\n","    def generate(self):\n","        dataset = tf.data.Dataset.from_tensor_slices((self.image_names, \n","                                                      np.array(self.record_list), \n","                                                      np.array(self.object_num_list)))\n","        dataset = dataset.shuffle(100000)\n","        dataset = dataset.map(self._data_preprocess, \n","                              num_parallel_calls = tf.data.experimental.AUTOTUNE)\n","        dataset = dataset.batch(BATCH_SIZE)\n","        dataset = dataset.prefetch(buffer_size=200)\n","\n","        return dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Model\n","* 使用DenseNet121當作base model，因為效果相對其他model較好，至於不使用DenseNet169, DenseNet201 是因為效果有限之外，訓練時間也會變長。\n","* 除了base model之外還保留了8層convolution layer，convolution layer中增加了Batch Normalization，其餘部分與原始code相同。\n","* Reference: https://arxiv.org/abs/1608.06993\n","* Reference: https://keras.io/api/applications/densenet/"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"v_GzXcmby5JM"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Zc5jGsjSy5JM"},"outputs":[],"source":["def conv_leaky_relu(inputs, filters, size, stride):\n","    x = layers.Conv2D(filters, size, stride, padding=\"same\")(inputs)\n","    x = layers.LeakyReLU(0.1)(x)\n","    x = layers.BatchNormalization()(x)\n","    return x"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"DTIsprUCy5JM"},"outputs":[],"source":["img_inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n","\n","x = tf.keras.applications.densenet.DenseNet121(include_top=False, weights=\"imagenet\")(img_inputs)\n","\n","x = conv_leaky_relu(x, 512, 1, 1)\n","x = conv_leaky_relu(x, 1024, 3, 1)\n","x = conv_leaky_relu(x, 512, 1, 1)\n","x = conv_leaky_relu(x, 1024, 3, 1)\n","x = conv_leaky_relu(x, 1024, 3, 1)\n","x = conv_leaky_relu(x, 1024, 3, 2)\n","x = conv_leaky_relu(x, 1024, 3, 1)\n","x = conv_leaky_relu(x, 1024, 3, 1)\n","\n","x = layers.Flatten()(x)\n","x = layers.Dense(4096, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.01))(x)\n","x = layers.LeakyReLU(0.1)(x)\n","outputs = layers.Dense(1470, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.01))(x)\n","\n","YOLO = keras.Model(inputs=img_inputs, outputs=outputs, name=\"YOLO\")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"sZ8r7ehLy5JM","outputId":"6d4a0b72-f2d5-4232-fe61-4dc9080ffdf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"YOLO\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 448, 448, 3)]     0         \n","_________________________________________________________________\n","densenet121 (Functional)     (None, None, None, 1024)  7037504   \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, 14, 14, 512)       524800    \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 14, 14, 512)       2048      \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 14, 14, 1024)      4719616   \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 1024)      0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 14, 14, 1024)      4096      \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 14, 14, 512)       524800    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 14, 14, 512)       2048      \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 14, 14, 1024)      4719616   \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 1024)      0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 14, 14, 1024)      4096      \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 14, 14, 1024)      9438208   \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 1024)      0         \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 14, 14, 1024)      4096      \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 7, 7, 1024)        9438208   \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 7, 7, 1024)        0         \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 7, 7, 1024)        4096      \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 7, 7, 1024)        9438208   \n","_________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 1024)        0         \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 7, 7, 1024)        4096      \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 7, 7, 1024)        9438208   \n","_________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 1024)        0         \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 7, 7, 1024)        4096      \n","_________________________________________________________________\n","flatten (Flatten)            (None, 50176)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 4096)              205524992 \n","_________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)    (None, 4096)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1470)              6022590   \n","=================================================================\n","Total params: 266,855,422\n","Trainable params: 266,757,438\n","Non-trainable params: 97,984\n","_________________________________________________________________\n"]}],"source":["YOLO.summary()"]},{"cell_type":"markdown","metadata":{"id":"CRhCN9riy5JM"},"source":["### Define Loss"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"NppnqVjRy5JM"},"outputs":[],"source":["# base boxes (for loss calculation)\n","base_boxes = np.zeros([CELL_SIZE, CELL_SIZE, 4])\n","\n","# initializtion for each cell\n","for y in range(CELL_SIZE):\n","    for x in range(CELL_SIZE):\n","        base_boxes[y, x, :] = [IMAGE_SIZE / CELL_SIZE * x, \n","                               IMAGE_SIZE / CELL_SIZE * y, 0, 0]\n","\n","base_boxes = np.resize(base_boxes, [CELL_SIZE, CELL_SIZE, 1, 4])\n","base_boxes = np.tile(base_boxes, [1, 1, BOXES_PER_CELL, 1])"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"lHrM0shky5JN"},"outputs":[],"source":["def iou(boxes1, boxes2):\n","    \"\"\"calculate ious\n","    Args:\n","      boxes1: 4-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n","      boxes2: 1-D tensor [4] ===> (x_center, y_center, w, h)\n","\n","    Return:\n","      iou: 3-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n","      ====> iou score for each cell\n","    \"\"\"\n","\n","    #boxes1 : [4(xmin, ymin, xmax, ymax), cell_size, cell_size, boxes_per_cell]\n","    boxes1 = tf.stack([boxes1[:, :, :, 0] - boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] - boxes1[:, :, :, 3] / 2,\n","                      boxes1[:, :, :, 0] + boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] + boxes1[:, :, :, 3] / 2])\n","\n","    #boxes1 : [cell_size, cell_size, boxes_per_cell, 4(xmin, ymin, xmax, ymax)]\n","    boxes1 = tf.transpose(boxes1, [1, 2, 3, 0])\n","\n","    boxes2 =  tf.stack([boxes2[0] - boxes2[2] / 2, boxes2[1] - boxes2[3] / 2,\n","                      boxes2[0] + boxes2[2] / 2, boxes2[1] + boxes2[3] / 2])\n","\n","    #calculate the left up point of boxes' overlap area\n","    lu = tf.maximum(boxes1[:, :, :, 0:2], boxes2[0:2])\n","    #calculate the right down point of boxes overlap area\n","    rd = tf.minimum(boxes1[:, :, :, 2:], boxes2[2:])\n","\n","    #intersection\n","    intersection = rd - lu \n","\n","    #the size of the intersection area\n","    inter_square = intersection[:, :, :, 0] * intersection[:, :, :, 1]\n","\n","    mask = tf.cast(intersection[:, :, :, 0] > 0, tf.float32) * tf.cast(intersection[:, :, :, 1] > 0, tf.float32)\n","\n","    #if intersection is negative, then the boxes don't overlap\n","    inter_square = mask * inter_square\n","\n","    #calculate the boxs1 square and boxs2 square\n","    square1 = (boxes1[:, :, :, 2] - boxes1[:, :, :, 0]) * (boxes1[:, :, :, 3] - boxes1[:, :, :, 1])\n","    square2 = (boxes2[2] - boxes2[0]) * (boxes2[3] - boxes2[1])\n","\n","    return inter_square/(square1 + square2 - inter_square + 1e-6)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def losses_calculation(predict, label):\n","    \"\"\"\n","    calculate loss\n","    Args:\n","      predict: 3-D tensor [cell_size, cell_size, num_classes + 5 * boxes_per_cell]\n","      label : [1, 5]  (x_center, y_center, w, h, class)\n","    \"\"\"\n","    label = tf.reshape(label, [-1])\n","\n","    #Step A. calculate objects tensor [CELL_SIZE, CELL_SIZE]\n","    #turn pixel position into cell position (corner)\n","    min_x = (label[0] - label[2] / 2) / (IMAGE_SIZE / CELL_SIZE)\n","    max_x = (label[0] + label[2] / 2) / (IMAGE_SIZE / CELL_SIZE)\n","\n","    min_y = (label[1] - label[3] / 2) / (IMAGE_SIZE / CELL_SIZE)\n","    max_y = (label[1] + label[3] / 2) / (IMAGE_SIZE / CELL_SIZE)\n","\n","    min_x = tf.floor(min_x)\n","    min_y = tf.floor(min_y)\n","\n","    max_x = tf.minimum(tf.math.ceil(max_x), CELL_SIZE)\n","    max_y = tf.minimum(tf.math.ceil(max_y), CELL_SIZE)\n","    \n","    #calculate mask of object with cells\n","    onset = tf.cast(tf.stack([max_y - min_y, max_x - min_x]), dtype=tf.int32)\n","    object_mask = tf.ones(onset, tf.float32)\n","\n","    offset = tf.cast(tf.stack([min_y, CELL_SIZE - max_y, min_x, CELL_SIZE - max_x]), tf.int32)\n","    offset = tf.reshape(offset, (2, 2))\n","    object_mask = tf.pad(object_mask, offset, \"CONSTANT\")\n","\n","    #Step B. calculate the coordination of object center and the corresponding mask\n","    #turn pixel position into cell position (center)\n","    center_x = label[0] / (IMAGE_SIZE / CELL_SIZE)\n","    center_x = tf.floor(center_x)\n","\n","    center_y = label[1] / (IMAGE_SIZE / CELL_SIZE)\n","    center_y = tf.floor(center_y)\n","\n","    response = tf.ones([1, 1], tf.float32)\n","\n","    #calculate the coordination of object center with cells\n","    objects_center_coord = tf.cast(tf.stack([center_y, CELL_SIZE - center_y - 1, \n","                             center_x, CELL_SIZE - center_x - 1]), \n","                             tf.int32)\n","    objects_center_coord = tf.reshape(objects_center_coord, (2, 2))\n","\n","    #make mask\n","    response = tf.pad(response, objects_center_coord, \"CONSTANT\")\n","\n","    #Step C. calculate iou_predict_truth [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n","    predict_boxes = predict[:, :, NUM_CLASSES + BOXES_PER_CELL:]\n","\n","    predict_boxes = tf.reshape(predict_boxes, [CELL_SIZE, \n","                                               CELL_SIZE, \n","                                               BOXES_PER_CELL, 4])\n","    #cell position to pixel position\n","    predict_boxes = predict_boxes * [IMAGE_SIZE / CELL_SIZE, \n","                                     IMAGE_SIZE / CELL_SIZE, \n","                                     IMAGE_SIZE, IMAGE_SIZE]\n","\n","    #if there's no predict_box in that cell, then the base_boxes will be calcuated with label and got iou equals 0\n","    predict_boxes = base_boxes + predict_boxes\n","\n","    iou_predict_truth = iou(predict_boxes, label[0:4])\n","\n","    #calculate C tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n","    C = iou_predict_truth * tf.reshape(response, [CELL_SIZE, CELL_SIZE, 1])\n","\n","    #calculate I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n","    I = iou_predict_truth * tf.reshape(response, [CELL_SIZE, CELL_SIZE, 1])\n","\n","    max_I = tf.reduce_max(I, 2, keepdims=True)\n","\n","    #replace large iou scores with response (object center) value\n","    I = tf.cast((I >= max_I), tf.float32) * tf.reshape(response, (CELL_SIZE, CELL_SIZE, 1))\n","\n","    #calculate no_I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n","    no_I = tf.ones_like(I, dtype=tf.float32) - I\n","\n","    p_C = predict[:, :, NUM_CLASSES:NUM_CLASSES + BOXES_PER_CELL]\n","\n","    #calculate truth x, y, sqrt_w, sqrt_h 0-D\n","    x = label[0]\n","    y = label[1]\n","\n","    sqrt_w = tf.sqrt(tf.abs(label[2]))\n","    sqrt_h = tf.sqrt(tf.abs(label[3]))\n","\n","    #calculate predict p_x, p_y, p_sqrt_w, p_sqrt_h 3-D [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n","    p_x = predict_boxes[:, :, :, 0]\n","    p_y = predict_boxes[:, :, :, 1]\n","\n","    p_sqrt_w = tf.sqrt(tf.minimum(IMAGE_SIZE * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 2])))\n","    p_sqrt_h = tf.sqrt(tf.minimum(IMAGE_SIZE * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 3])))\n","\n","    #calculate ground truth p 1-D tensor [NUM_CLASSES]\n","    P = tf.one_hot(tf.cast(label[4], tf.int32), NUM_CLASSES, dtype=tf.float32)\n","\n","    #calculate predicted p_P 3-D tensor [CELL_SIZE, CELL_SIZE, NUM_CLASSES]\n","    p_P = predict[:, :, 0:NUM_CLASSES]\n","\n","    #class_loss\n","    class_loss = tf.nn.l2_loss(tf.reshape(object_mask, (CELL_SIZE, CELL_SIZE, 1)) * (p_P - P)) * CLASS_SCALE\n","\n","    #object_loss\n","    object_loss = tf.nn.l2_loss(I * (p_C - C)) * OBJECT_SCALE\n","\n","    #noobject_loss\n","    noobject_loss = tf.nn.l2_loss(no_I * (p_C)) * NOOBJECT_SCALE\n","\n","    #coord_loss\n","    coord_loss = (tf.nn.l2_loss(I * (p_x - x)/(IMAGE_SIZE/CELL_SIZE)) +\n","                  tf.nn.l2_loss(I * (p_y - y)/(IMAGE_SIZE/CELL_SIZE)) +\n","                  tf.nn.l2_loss(I * (p_sqrt_w - sqrt_w))/IMAGE_SIZE +\n","                  tf.nn.l2_loss(I * (p_sqrt_h - sqrt_h))/IMAGE_SIZE) * COORD_SCALE\n","\n","    return class_loss + object_loss + noobject_loss + coord_loss"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"2KD09mfry5JN"},"outputs":[],"source":["def yolo_loss(predicts, labels, objects_num):\n","    \"\"\"\n","    Add Loss to all the trainable variables\n","    Args:\n","        predicts: 4-D tensor [batch_size, cell_size, cell_size, num_classes + 5 * boxes_per_cell]\n","        ===> (num_classes, boxes_per_cell, 4 * boxes_per_cell)\n","        labels  : 3-D tensor of [batch_size, max_objects, 5]\n","        objects_num: 1-D tensor [batch_size]\n","    \"\"\"\n","\n","    loss = 0.\n","    \n","    #you can parallel the code with tf.map_fn or tf.vectorized_map (big performance gain!)\n","    for i in tf.range(predicts.shape[0]):\n","        predict = predicts[i, :, :, :]\n","        label = labels[i, :, :]\n","        object_num = objects_num[i]\n","\n","        for j in tf.range(object_num):\n","            results = losses_calculation(predict, label[j:j+1, :])\n","            loss = loss + results\n","\n","    return loss/predicts.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"jwypCP4Sy5JN"},"source":["### Training\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"GFgkb9Xjy5JN"},"outputs":[],"source":["dataset = DatasetGenerator().generate()\n","optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n","train_loss_metric = tf.keras.metrics.Mean(name='loss')"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"tf804NLqy5JN"},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe4ee1ce2e8>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["ckpt = tf.train.Checkpoint(epoch=tf.Variable(0), net=YOLO)\n","manager = tf.train.CheckpointManager(ckpt, './ckpts/YOLO', max_to_keep=5, checkpoint_name='yolo')\n","\n","ckpt.restore('./ckpts/YOLO/yolo-146')\n","# ckpt.restore(manager.latest_checkpoint)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"WVdivd6Cy5JN"},"outputs":[],"source":["@tf.function\n","def train_step(image, labels, objects_num):\n","    with tf.GradientTape() as tape:\n","        outputs = YOLO(image)\n","        class_end = CELL_SIZE * CELL_SIZE * NUM_CLASSES\n","        conf_end = class_end + CELL_SIZE * CELL_SIZE * BOXES_PER_CELL\n","        class_probs = tf.reshape(outputs[:, 0:class_end], (-1, 7, 7, 20))\n","        confs = tf.reshape(outputs[:, class_end:conf_end], (-1, 7, 7, 2))\n","        boxes = tf.reshape(outputs[:, conf_end:], (-1, 7, 7, 2*4))\n","        predicts = tf.concat([class_probs, confs, boxes], 3)\n","\n","        loss = yolo_loss(predicts, labels, objects_num)\n","        train_loss_metric(loss)\n","\n","    grads = tape.gradient(loss, YOLO.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, YOLO.trainable_weights))"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"R6FBd-vXy5JN","outputId":"65bcf843-743d-4b5f-ddfe-fdec13adfd36"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-11-19 16:06:21.482863, start training.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1185/1185 [07:27<00:00,  2.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["2023-11-19 16:13:49.408052, Epoch 1: loss 1.77\n","Saved checkpoint for epoch 147: ./ckpts/YOLO/yolo-147\n"]}],"source":["from tqdm import tqdm\n","from datetime import datetime\n","\n","print(\"{}, start training.\".format(datetime.now()))\n","for i in range(EPOCHS):\n","    train_loss_metric.reset_states()\n","    ckpt.epoch.assign_add(1)\n","\n","    for idx, (image, labels, objects_num) in enumerate(tqdm(dataset)):\n","        train_step(image, labels, objects_num)\n","\n","    print(\"{}, Epoch {}: loss {:.2f}\".format(datetime.now(), i+1, train_loss_metric.result()))\n","\n","    save_path = manager.save()\n","    print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.epoch), save_path))  "]},{"cell_type":"markdown","metadata":{},"source":["### Predict Test data\n","* 處理output從選擇最高的confidence score，改成只要confidence score有超過最高confidence score * threshold就輸出結果。\n","* 在觀察預測結果時，把threshold設成0.5可以讓confidence score可以得到更好的結果，將threshold調高至0.6和0.7，但效果反而變差。"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"iUc3Pt2uy5JO"},"outputs":[],"source":["def process_outputs(outputs):\n","    \"\"\"\n","    Process YOLO outputs into bou\n","    \"\"\"\n","\n","    class_end = CELL_SIZE * CELL_SIZE * NUM_CLASSES\n","    conf_end = class_end + CELL_SIZE * CELL_SIZE * BOXES_PER_CELL\n","    class_probs = np.reshape(outputs[:, 0:class_end], (-1, 7, 7, 20))\n","    confs = np.reshape(outputs[:, class_end:conf_end], (-1, 7, 7, 2))\n","    boxes = np.reshape(outputs[:, conf_end:], (-1, 7, 7, 2 * 4))\n","    predicts = np.concatenate([class_probs, confs, boxes], 3)\n","\n","    p_classes = predicts[0, :, :, 0:20]\n","    C = predicts[0, :, :, 20:22]\n","    coordinate = predicts[0, :, :, 22:]\n","\n","    p_classes = np.reshape(p_classes, (CELL_SIZE, CELL_SIZE, 1, 20))\n","    C = np.reshape(C, (CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 1))\n","\n","    P = C * p_classes\n","    # P's shape [7, 7, 2, 20]\n","\n","    xmin, ymin, xmax, ymax, class_num, conf = [], [], [], [], [], []\n","\n","    max_conf_idx = np.unravel_index(np.argmax(P), P.shape)\n","    threshold = P[max_conf_idx] * 0.5\n","\n","    for i in range(np.prod(P.shape)):\n","        idx = np.unravel_index(i, P.shape)\n","        if (P[idx] > threshold):\n","            class_num.append(idx[3])\n","            conf.append(P[idx])\n","\n","            coordinate = np.reshape(coordinate, (CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4))\n","            xcenter, ycenter, w, h = coordinate[idx[0], idx[1], idx[2], :]\n","\n","            xcenter = (idx[1] + xcenter) * (IMAGE_SIZE / float(CELL_SIZE))\n","            ycenter = (idx[0] + ycenter) * (IMAGE_SIZE / float(CELL_SIZE))\n","\n","            w = w * IMAGE_SIZE\n","            h = h * IMAGE_SIZE\n","\n","            xmin.append(xcenter - w / 2.0)\n","            ymin.append(ycenter - h / 2.0)\n","\n","            xmax.append(xmin[-1] + w)\n","            ymax.append(ymin[-1] + h)\n","\n","    return xmin, ymin, xmax, ymax, class_num, conf"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"zbLcEBHEy5JO"},"outputs":[],"source":["test_img_files = open('./dataset/pascal_voc_testing_data.txt')\n","test_img_dir = './dataset/VOCdevkit_test/VOC2007/JPEGImages/'\n","test_images = []\n","\n","for line in test_img_files:\n","    line = line.strip()\n","    ss = line.split(' ')\n","    test_images.append(ss[0])\n","\n","\n","\n","def load_img_data(image_name):\n","    image_file = tf.io.read_file(test_img_dir+image_name)\n","    image = tf.image.decode_jpeg(image_file, channels=3)\n","\n","    h = tf.shape(image)[0]\n","    w = tf.shape(image)[1]\n","\n","    image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])\n","    image = tf.keras.applications.densenet.preprocess_input(image)\n","\n","    return image_name, image, h, w\n","\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices(test_images)\\\n","                              .map(load_img_data, num_parallel_calls = tf.data.experimental.AUTOTUNE)\\\n","                              .batch(32)\\\n","                              .prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"a1nsvNTpy5JO","outputId":"39cae9e6-576c-4334-a948-ead55eb23c9b"},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe4ee1c8320>"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["ckpt = tf.train.Checkpoint(net=YOLO)\n","ckpt.restore('./ckpts/YOLO/yolo-147')"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"CpIjmiiuy5JO"},"outputs":[],"source":["@tf.function\n","def prediction_step(img):\n","    return YOLO(img, training=False)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"6iu6w8Byy5JO"},"outputs":[],"source":["output_file = open('./test_prediction_with_aug.txt', 'w')\n","\n","for img_name, test_img, img_h, img_w in test_dataset:\n","    batch_num = img_name.shape[0]\n","    for i in range(batch_num):\n","        xmin, ymin, xmax, ymax, class_num, conf = process_outputs(prediction_step(test_img[i:i+1]))\n","        for j in range(len(xmin)):\n","            xmin[j] = xmin[j] * (img_w[i] / IMAGE_SIZE)\n","            ymin[j] = ymin[j] * (img_h[i] / IMAGE_SIZE)\n","            xmax[j] = xmax[j] * (img_w[i] / IMAGE_SIZE)\n","            ymax[j] = ymax[j] * (img_h[i] / IMAGE_SIZE)\n","            output_file.write(img_name[i].numpy().decode('ascii')+\" %d %d %d %d %d %f\\n\"%(\n","                xmin[j], ymin[j], xmax[j], ymax[j], class_num[j], conf[j]))\n","                \n","output_file.close()"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mBhMgkvky5JO"},"outputs":[],"source":["import sys\n","sys.path.insert(0, './evaluate')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"eVVldDaXy5JO","outputId":"ee2653b6-428e-430f-d72d-ac1ba8b41616"},"outputs":[{"name":"stderr","output_type":"stream","text":["./evaluate/evaluate.py:814: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography. The next release of cryptography will remove support for Python 3.6.\n","  from cryptography.fernet import Fernet\n"]},{"name":"stdout","output_type":"stream","text":["End Evalutation\n","score: 0.376731\n"]}],"source":["import evaluate\n","import pandas as pd\n","\n","evaluate.evaluate('./test_prediction_with_aug.txt', './output_file_with_aug.csv')\n","\n","cap = pd.read_csv('./output_file_with_aug.csv')['packedCAP']\n","print('score: {:f}'.format(sum((1. - cap) ** 2) / len(cap)))"]},{"cell_type":"markdown","metadata":{},"source":["### Conclusion\n","這次Competition剛開始的時候先把助教的code跑完後發現成績離80的門檻有段距離，所以會覺得必須要實作比較新的model架構才能夠把分數拉上去，不過後續在對data進行處後發現成績有顯著的提升，感覺起來跟Competition 1一樣，再次印證到training data的好壞會大幅影響結果，最後結合很方式讓成績突破80的門檻，想要再往上提升大概就需要靠更改model架構才有辦法，不過由於這次的時間與其他課程作業有衝突，就沒有繼續嘗試了。\n","\n","至於遇到的困難就是如何在訓練資料量和訓練時間做trade-off，因為這次訓練資料相比於作業和Competition 1多更多，最後的解法就是只能減少Augmentation的數量。\n","\n","最後一點就是體會到顯卡對於DL的重要性，一開始使用1080ti做訓練的時候不只是速度相對較慢，記憶體也比較少，導致batch size和model不能太大，後還換到4090後能夠明顯感覺到差距。"]}],"metadata":{"colab":{"provenance":[{"file_id":"1r3rlgVC9N3EzRYdHfD_FsESBOpQ74Yv0","timestamp":1700283592005}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}
